import os
import json
import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertModel
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from tqdm import tqdm
import joblib
import warnings
warnings.filterwarnings("ignore")

# âœ… 1. ëª¨ë¸ ì •ì˜
class BERTClassifier(nn.Module):
    def __init__(self, num_classes, dropout=0.3):
        super(BERTClassifier, self).__init__()
        self.bert = BertModel.from_pretrained("snunlp/KR-FinBERT")
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        return self.fc(self.dropout(pooled_output))

# âœ… 2. ë°ì´í„°ì…‹ í´ë˜ìŠ¤
class NewsDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        tokens = self.tokenizer(
            self.texts[idx],
            padding="max_length",
            truncation=True,
            max_length=self.max_len,
            return_tensors="pt"
        )
        input_ids = tokens["input_ids"].squeeze(0)
        attention_mask = tokens["attention_mask"].squeeze(0)
        return input_ids, attention_mask, torch.tensor(self.labels[idx])

# âœ… 3. í•™ìŠµ í•¨ìˆ˜
def train_all_5class(
    jsonl_path,
    model_dir="../saved_model",
    epochs_to_train=5
):
    os.makedirs(model_dir, exist_ok=True)

    model_path = os.path.join(model_dir, "krfinbert_stock_classifier_5class.pth")
    tokenizer_path = os.path.join(model_dir, "bert_tokenizer.pkl")
    epoch_info_path = os.path.join(model_dir, "epoch_info.json")

    # âœ… ë°ì´í„° ë¡œë”©
    df = pd.read_json(jsonl_path, lines=True)
    df["text"] = df["title"] + " " + df["summary"]
    df["label"] = df["labeling_5class"]  # âœ… ì»¬ëŸ¼ëª… ì •í™•íˆ ì‚¬ìš©
    df = df.sample(frac=1, random_state=42).reset_index(drop=True)

    # âœ… train/val split (ë¹„ìœ¨ ìœ ì§€)
    train_texts, val_texts, train_labels, val_labels = train_test_split(
        df["text"].tolist(),
        df["label"].tolist(),
        test_size=0.2,
        random_state=42,
        stratify=df["label"]
    )

    tokenizer = BertTokenizer.from_pretrained("snunlp/KR-FinBERT")
    joblib.dump(tokenizer, tokenizer_path)

    train_loader = DataLoader(NewsDataset(train_texts, train_labels, tokenizer), batch_size=32, shuffle=True)
    val_loader = DataLoader(NewsDataset(val_texts, val_labels, tokenizer), batch_size=32)

    # âœ… MPS (Mac GPU) ìš°ì„  ì ìš©
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        print("âœ… MPS (Mac GPU) ì‚¬ìš© ì¤‘")
    elif torch.cuda.is_available():
        device = torch.device("cuda")
        print("âœ… CUDA ì‚¬ìš© ì¤‘")
    else:
        device = torch.device("cpu")
        print("âš ï¸ GPU ì‚¬ìš© ë¶ˆê°€ - CPU ì‚¬ìš© ì¤‘")

    model = BERTClassifier(num_classes=5).to(device)

    # âœ… ì´ì–´ì„œ í•™ìŠµ ê°€ëŠ¥
    start_epoch = 0
    if os.path.exists(model_path):
        model.load_state_dict(torch.load(model_path, map_location=device))
        print("âœ… ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        if os.path.exists(epoch_info_path):
            with open(epoch_info_path, "r") as f:
                epoch_info = json.load(f)
                start_epoch = epoch_info.get("last_epoch", 0)
                print(f"ğŸ” ì´ì–´ì„œ í•™ìŠµ ì‹œì‘: epoch {start_epoch + 1}")

    optimizer = optim.AdamW(model.parameters(), lr=2e-5)
    criterion = nn.CrossEntropyLoss()

    for epoch in range(start_epoch, start_epoch + epochs_to_train):
        model.train()
        total_loss = 0
        print(f"\nğŸ”„ Epoch [{epoch + 1}/{start_epoch + epochs_to_train}]")

        progress_bar = tqdm(train_loader, desc=f"Training Epoch {epoch + 1}")
        for input_ids, attention_mask, labels in progress_bar:
            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(input_ids, attention_mask)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            progress_bar.set_postfix(loss=loss.item())

        print(f"âœ… í‰ê·  Loss: {total_loss / len(train_loader):.4f}")

        torch.save(model.state_dict(), model_path)
        with open(epoch_info_path, "w") as f:
            json.dump({"last_epoch": epoch + 1}, f)

        print(f"ğŸ“¦ ëª¨ë¸ ì €ì¥ ì™„ë£Œ (Epoch {epoch + 1})")

    # âœ… í‰ê°€
    model.eval()
    all_preds, all_labels = [], []
    with torch.no_grad():
        progress_bar = tqdm(val_loader, desc="Evaluating")
        for input_ids, attention_mask, labels in progress_bar:
            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)
            outputs = model(input_ids, attention_mask)
            preds = torch.argmax(outputs, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    print("\nğŸ¯ Accuracy:", accuracy_score(all_labels, all_preds))
    print(classification_report(all_labels, all_preds))

# âœ… ì‹¤í–‰
if __name__ == "__main__":
    train_all_5class(
        jsonl_path="../real_final_data_5class/labeled_data_5class_3.jsonl",
        epochs_to_train=5
    )
